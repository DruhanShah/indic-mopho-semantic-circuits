paths:
  assets_dir: /scratch/druhan/anlp

seed: 42


language: en # en, hi, te

data:
  name: wikipedia
  date: "20231101"
  split: "train[:1%]"

tokenizer:
  vocab_size: 50257
  min_frequency: 2

# GPT-2 Small
model:
  n_ctx: 256
  n_embd: 768
  n_l: 12
  n_h: 12

processing:
  num_workers: 4

# Corresponds to Hugging Face"s transformers.TrainingArguments
training:
  num_train_epochs: 3
  per_device_train_batch_size: 8
  learning_rate: 5e-5

  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  lr_scheduler_type: "cosine" # "linear", "cosine", "constant", etc.
  warmup_steps: 500

  logging_strategy: "steps"
  logging_steps: 500
  save_strategy: "steps"
  save_steps: 10000
  save_total_limit: 2

  fp16: true
  
  prediction_loss_only: true
  report_to: "none" # "tensorboard", "wandb", etc.
  overwrite_output_dir: true
